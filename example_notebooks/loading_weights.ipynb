{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loading file\n",
    "[Source](http://files.fast.ai/models/wt103/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "\n",
    "sys.path.append('../')\n",
    "from training_utils import training_loop, test_loop\n",
    "from model import RNNLM\n",
    "from data_utils import (IndexVectorizer, \n",
    "                        TextDataset, \n",
    "                        SpacyTokenizer,\n",
    "                        LMDataLoader)\n",
    "from lr_scheduler import CyclicLR\n",
    "# from data_utils import IndexVectorizer, TextDataset, simple_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# Config\n",
    "####################################################\n",
    "\n",
    "## Input / output\n",
    "data_dir = '../../data/imdb'\n",
    "\n",
    "## Tokenization\n",
    "TOKENIZE = SpacyTokenizer().tokenize\n",
    "\n",
    "## Vectorization\n",
    "MIN_WORD_FREQ = 2\n",
    "MAX_VOCAB_SIZE = 20000\n",
    "STAT_END_TOK = True\n",
    "\n",
    "## Model Architecture\n",
    "dropout = 0.3\n",
    "lstm_tieweights = True\n",
    "\n",
    "## Training Language Model\n",
    "batch_size = 50\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 100\n",
    "display_epoch_freq = 10\n",
    "target_seq_len = 65\n",
    "max_seq_len = 75\n",
    "min_seq_len = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU variables\n",
    "use_gpu = torch.cuda.is_available()\n",
    "device_num = 0\n",
    "device = torch.device(f\"cuda:{device_num}\" if False else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading pre-trained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_pre_trained_models = '../../data/weights_pretrained/'\n",
    "new_data_directory = '../../data/imdb/models/'\n",
    "os.makedirs(directory_pre_trained_models, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget http://files.fast.ai/models/wt103/fwd_wt103_enc.h5 -O $directory_pre_trained_models/fwd_wt103_enc.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can skip this one V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget http://files.fast.ai/models/wt103/fwd_wt103.h5 -O $directory_pre_trained_models/fwd_wt103.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget http://files.fast.ai/models/wt103/itos_wt103.pkl -O $directory_pre_trained_models/fitos_wt103.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 886M\r\n",
      "drwxrwxr-x. 5 frido frido   56 Feb 12 21:38 ..\r\n",
      "-rwxr-xr-x. 1 root  root  441M Feb 12 17:18 fwd_wt103.h5\r\n",
      "drwxrwsr-x. 2 root  root    73 Feb 12 17:18 .\r\n",
      "-rwxr-xr-x. 1 root  root  441M Feb 12 17:18 fwd_wt103_enc.h5\r\n",
      "-rwxr-xr-x. 1 root  root  4.0M Feb 12 17:18 fitos_wt103.pkl\r\n"
     ]
    }
   ],
   "source": [
    "!ls -ltha $directory_pre_trained_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_file = os.path.join(directory_pre_trained_models, 'fwd_wt103.h5')\n",
    "fitos_file = os.path.join(directory_pre_trained_models, 'fitos_wt103.pkl')\n",
    "vectorizer_file = os.path.join(new_data_directory, 'lm_vectorizer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = torch.load(encoder_file, map_location=lambda storage, loc: storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = enc['0.encoder.weight'].shape[1]\n",
    "hidden_size = int(enc['0.rnns.0.module.weight_hh_l0_raw'].shape[0]/4)\n",
    "num_layers = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_enc = {}\n",
    "for k,v in enc.items():\n",
    "    layer_detail = k.split('.')\n",
    "    layer_name = layer_detail[-1].replace('_raw', '')\n",
    "    if len(layer_detail) == num_layers: \n",
    "        new_enc[f'{layer_detail[1]}.{layer_name}'] = v\n",
    "    else:\n",
    "        new_enc[f'{layer_detail[1]}.{layer_detail[2]}.{layer_name}'] = v\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove this odd element as it is the same as encoder.weight\n",
    "#new_enc['encoder_with_dropout.embed.weight'] == new_enc['encoder.weight']\n",
    "del new_enc['encoder_with_dropout.embed.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our vectorizer\n",
    "\n",
    "## Load the wikitext vocabulary\n",
    "pretrained_idx2word = pickle.load(open(fitos_file, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_word2idx = {k: i for i,k in enumerate(pretrained_idx2word)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model_vectorizer = pickle.load(open(vectorizer_file, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_encoder_weights = enc['0.encoder.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_m = pretrained_encoder_weights.mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_m = [x.item() for x in row_m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_vocab_size = len(new_model_vectorizer.word2idx)\n",
    "new_encoder_weights = torch.tensor([row_m for i in range(new_vocab_size)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_idx2weights = {}\n",
    "for word, i in new_model_vectorizer.word2idx.items():\n",
    "    if word in pretrained_word2idx:\n",
    "        word_idx = pretrained_word2idx[word]\n",
    "        new_encoder_weights[i] = pretrained_encoder_weights[word_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "new_enc['encoder.weight'] = new_encoder_weights\n",
    "new_enc['decoder.weight'] = copy.copy(new_encoder_weights)\n",
    "new_enc['decoder.bias'] = torch.zeros(new_enc['decoder.weight'].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of a model\n",
    "lm = RNNLM(device=device, vocab_size=new_vocab_size, \n",
    "           embedding_size=embedding_size, hidden_size=hidden_size, \n",
    "           batch_size=50, num_layers=3, tie_weights=True, word2idx = new_model_vectorizer.word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.load_state_dict(new_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IO setup\n",
    "today = datetime.datetime.now().strftime('%Y-%m-%d')\n",
    "model_cache_dir = os.path.join(data_dir, 'models')\n",
    "data_cache = os.path.join(model_cache_dir, 'data_cache.pkl')\n",
    "vectorizer_cache = os.path.join(model_cache_dir, 'lm_vectorizer.pkl')\n",
    "os.makedirs(model_cache_dir, exist_ok=True)\n",
    "model_file_lm = os.path.join(model_cache_dir, f'LM__{today}.json')\n",
    "model_file_class = os.path.join(model_cache_dir, f'CLASS__{today}.json')\n",
    "\n",
    "train_file = os.path.join(data_dir, 'train.csv')\n",
    "valid_file = os.path.join(data_dir, 'valid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "RE_VECTORIZE = False\n",
    "if RE_VECTORIZE or not os.path.isfile(data_cache):\n",
    "    train = pd.read_csv(train_file)\n",
    "    valid = pd.read_csv(valid_file)\n",
    "    vectorizer = IndexVectorizer(max_words = MAX_VOCAB_SIZE, \n",
    "                             min_frequency=MIN_WORD_FREQ,\n",
    "                             start_end_tokens=STAT_END_TOK, \n",
    "                             tokenize=TOKENIZE)\n",
    "    train_ds = TextDataset(data=train, vectorizer=vectorizer, text_col='text')\n",
    "    valid_ds = TextDataset(data=valid, vectorizer=vectorizer, text_col='text')\n",
    "    pickle.dump([train_ds, valid_ds], open(data_cache, 'wb'))\n",
    "    pickle.dump(vectorizer, open(vectorizer_cache, 'wb'))\n",
    "else:\n",
    "    train_ds, valid_ds = pickle.load(open(data_cache, 'rb'))\n",
    "    vectorizer = pickle.load(open(vectorizer_cache, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = LMDataLoader(dataset=train_ds, \n",
    "                        target_seq_len=target_seq_len, \n",
    "                        shuffle=True, \n",
    "                        max_seq_len=max_seq_len, \n",
    "                        min_seq_len=min_seq_len, \n",
    "                        p_half_seq_len=0.05,\n",
    "                        batch_size=batch_size)\n",
    "valid_dl = LMDataLoader(dataset=valid_ds,\n",
    "                        target_seq_len=target_seq_len, \n",
    "                        shuffle=True, \n",
    "                        max_seq_len=max_seq_len, \n",
    "                        min_seq_len=min_seq_len, \n",
    "                        p_half_seq_len=0.05,\n",
    "                        batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_gpu: torch.cuda.manual_seed(303)\n",
    "else: torch.manual_seed(303)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up Files to save stuff in\n",
    "runtime = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58096fcb60f04cf28934cf98fe08d62f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0000; Train-Loss: 5.4269; Val-Loss 4.5166; Train-accuracy 0.1604; Val-Accuracy 0.2362\n",
      "Sample: the actors in this movie are superb and the character leads the film effectively . the cast is great with mann . i think it was a great movie and really is one of its best work of all time . the music is fantastic . <END> <START> damaged by nature . it ramtha because\n",
      "Epoch: 0001; Train-Loss: 5.0750; Val-Loss 4.4167; Train-accuracy 0.1872; Val-Accuracy 0.2446\n",
      "Sample: the actors in this movie . those bad guys could get the impression of drama and this one gets huge incredible mystery plot . i mean /><br />its , we can be rewarded and eat everyone who will expect it to do . i thought they would have been playing the roles of zombie reading\n",
      "-----------------------------------------------------------------------------------------\n",
      "Exiting from training early\n",
      "Lowest loss: 4.4167\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if use_gpu:\n",
    "    lm = lm.to(device)\n",
    "    \n",
    "# Loss and Optimizer\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "# Extract pointers to the parameters of the lstms\n",
    "param_list = [{'params': rnn.parameters(), 'lr': learning_rate} for rnn in lm.rnns]\n",
    "\n",
    "# If weights are tied between encoder and decoder, we can only optimize \n",
    "# parameters in one of those two layers\n",
    "if not lstm_tieweights:\n",
    "    param_list.extend([\n",
    "            {'params': lm.encoder.parameters(), 'lr':learning_rate},\n",
    "            {'params': lm.decoder.parameters(), 'lr':learning_rate},\n",
    "        ])\n",
    "else:\n",
    "    param_list.extend([\n",
    "        {'params': lm.decoder.parameters(), 'lr':learning_rate},\n",
    "    ])\n",
    "\n",
    "optimizer = torch.optim.Adam(param_list, lr=learning_rate)\n",
    "\n",
    "scheduler = CyclicLR(optimizer,  max_lrs=[0.1, 0.1, 0.1, 0.1, 0.1], \n",
    "                     mode='ulmfit', ratio=1.5, cut_frac=0.4, \n",
    "                     n_epochs=num_epochs, batchsize=50000/1171, \n",
    "                     verbose=False, epoch_length=50000)\n",
    "\n",
    "history = training_loop(batch_size=batch_size, \n",
    "                        num_epochs=num_epochs,\n",
    "                        display_freq=1, \n",
    "                        model=lm, \n",
    "                        criterion=loss,\n",
    "                        optim=optimizer,\n",
    "                        scheduler=None,\n",
    "                        device=device,\n",
    "                        training_set=train_dl,\n",
    "                        validation_set=valid_dl,\n",
    "                        best_model_path=model_file_lm,\n",
    "                        history=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# # load weights\n",
    "# for k, v in enc.items():    \n",
    "#     layer_detail = k.split('.')\n",
    "#     layer_name = layer_detail[-1].replace('_raw', '')\n",
    "    \n",
    "#     print(layer_detail)\n",
    "\n",
    "#     try:\n",
    "#         layer = getattr(lm, layer_detail[1])\n",
    "#     except Exception as e:\n",
    "#         print(e)\n",
    "        \n",
    "#     # which rnn, and what layer???\n",
    "#     if layer_detail[1] == 'rnns':\n",
    "#         n_rnn = int(layer_detail[2])\n",
    "#         layer = layer[n_rnn]\n",
    "#     try:\n",
    "#         # this is what assigns the new value\n",
    "#         getattr(layer, layer_name).data = v\n",
    "    \n",
    "#     except Exception as e:\n",
    "#         print(e)(layer_detail)\n",
    "\n",
    "for k,v in lm.idx2word.items():\n",
    "    if v==\"<UNK>\":\n",
    "        print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
