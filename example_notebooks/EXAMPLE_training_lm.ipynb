{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "sys.path.append('../')\n",
    "from lr_scheduler import CyclicLR\n",
    "from training_utils import training_loop, test_loop\n",
    "from model import RNNLM\n",
    "from data_utils import (IndexVectorizer, \n",
    "                        TextDataset, \n",
    "                        SpacyTokenizer,\n",
    "                        LMDataLoader)\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "# Config\n",
    "####################################################\n",
    "\n",
    "## Input / output\n",
    "data_dir = '../../../..'\n",
    "\n",
    "## Tokenization\n",
    "TOKENIZE = SpacyTokenizer().tokenize\n",
    "\n",
    "## Vectorization\n",
    "MIN_WORD_FREQ = 2\n",
    "MAX_VOCAB_SIZE = 20000\n",
    "STAT_END_TOK = True\n",
    "\n",
    "## Model Architecture\n",
    "hidden_dim = 300\n",
    "embedding_dim = 300\n",
    "dropout = 0.5\n",
    "lstm_layers = 3\n",
    "lstm_bidirection = True\n",
    "\n",
    "## Training Language Model\n",
    "batch_size = 80\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 100\n",
    "display_epoch_freq = 10\n",
    "target_seq_len = 100\n",
    "max_seq_len = 130\n",
    "min_seq_len = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GPU setup\n",
    "use_gpu = torch.cuda.is_available()\n",
    "device_num = 0\n",
    "device = torch.device(f\"cuda:{device_num}\" if use_gpu else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IO setup\n",
    "today = datetime.datetime.now().strftime('%Y-%m-%d')\n",
    "model_cache_dir = os.path.join(data_dir, 'models')\n",
    "data_cache = os.path.join(model_cache_dir, 'data_cache.pkl')\n",
    "vectorizer_cache = os.path.join(model_cache_dir, 'lm_vectorizer.pkl')\n",
    "os.makedirs(model_cache_dir, exist_ok=True)\n",
    "model_file_lm = os.path.join(model_cache_dir, f'LM__{today}.json')\n",
    "model_file_class = os.path.join(model_cache_dir, f'CLASS__{today}.json')\n",
    "\n",
    "train_file = os.path.join(data_dir, 'train.csv')\n",
    "valid_file = os.path.join(data_dir, 'valid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "RE_VECTORIZE = False\n",
    "if RE_VECTORIZE or not os.path.isfile(data_cache):\n",
    "    train = pd.read_csv(train_file)\n",
    "    valid = pd.read_csv(valid_file)\n",
    "    vectorizer = IndexVectorizer(max_words = MAX_VOCAB_SIZE, \n",
    "                             min_frequency=MIN_WORD_FREQ,\n",
    "                             start_end_tokens=STAT_END_TOK, \n",
    "                             tokenize=TOKENIZE)\n",
    "    train_ds = TextDataset(data=train, vectorizer=vectorizer, text_col='text')\n",
    "    valid_ds = TextDataset(data=valid, vectorizer=vectorizer, text_col='text')\n",
    "    pickle.dump([train_ds, valid_ds], open(data_cache, 'wb'))\n",
    "    pickle.dump(vectorizer, open(vectorizer_cache, 'wb'))\n",
    "else:\n",
    "    train_ds, valid_ds = pickle.load(open(data_cache, 'rb'))\n",
    "    vectorizer = pickle.load(open(vectorizer_cache, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 5000\n",
      "valid size: 1000\n",
      "Vocab size: 11665\n"
     ]
    }
   ],
   "source": [
    "print(f'Train size: {len(train_ds)}\\nvalid size: {len(valid_ds)}')\n",
    "print(f\"Vocab size: {len(vectorizer.vocabulary)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = LMDataLoader(dataset=train_ds, \n",
    "                        target_seq_len=target_seq_len, \n",
    "                        shuffle=True, \n",
    "                        max_seq_len=max_seq_len, \n",
    "                        min_seq_len=min_seq_len, \n",
    "                        p_half_seq_len=0.05,\n",
    "                        batch_size=batch_size)\n",
    "valid_dl = LMDataLoader(dataset=valid_ds,\n",
    "                        target_seq_len=target_seq_len, \n",
    "                        shuffle=True, \n",
    "                        max_seq_len=max_seq_len, \n",
    "                        min_seq_len=min_seq_len, \n",
    "                        p_half_seq_len=0.05,\n",
    "                        batch_size=batch_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_gpu: torch.cuda.manual_seed(303)\n",
    "else: torch.manual_seed(303)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up Files to save stuff in\n",
    "runtime = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "model_file_lm = model_file_lm\n",
    "    \n",
    "TIE_WEIGHTS = True \n",
    "NUM_LAYERS = 3\n",
    "BIDRIECTIONAL = True\n",
    "\n",
    "# Build and initialize the model\n",
    "lm = RNNLM(device, vectorizer.vocabulary_size, embedding_dim, hidden_dim, batch_size, \n",
    "           dropout = dropout, \n",
    "           tie_weights = TIE_WEIGHTS, \n",
    "           num_layers = NUM_LAYERS, \n",
    "           bidirectional = BIDRIECTIONAL, \n",
    "           word2idx = vectorizer.word2idx,\n",
    "           log_softmax = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_gpu:\n",
    "    lm = lm.to(device)\n",
    "#lm.init_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c41c03857714e8ea543009dc426014d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0000; Loss: 6.7565; Val-Loss 6.2569; Perplexity 859.5862; Val-Perplexity 521.5804\n",
      "Sample: this is good most hour . take started bite a ever better know . everywhere , a because went local it dish a und have a purchase could upon walked he they next <END> worth \n",
      " live beer heard desired were she \n",
      "\n",
      " ! fried fresh great change the come any appetizer combination\n",
      "Epoch: 0001; Loss: 6.1147; Val-Loss 5.3322; Perplexity 452.4729; Val-Perplexity 206.8927\n",
      "Sample: this is good usually - even on decent and a my great the maybe for a sure came and lack to great ! wait time although that i 've lunch needed i little is tenders and or work will edges sauce by my done and salon should reason room a very what not\n",
      "Epoch: 0002; Loss: 5.4599; Val-Loss 4.6182; Perplexity 235.0816; Val-Perplexity 101.3119\n",
      "Sample: this is good there $ refreshing find \n",
      "\n",
      " everything from been ! from <END> while and in definitely when \n",
      " way support recommended you my our be cool arrived stays step my seems ) one interested and even to god tipped not unprofessional the going put did awesome   each really groupon attempts\n",
      "Epoch: 0003; Loss: 4.9610; Val-Loss 4.1531; Perplexity 142.7345; Val-Perplexity 63.6303\n",
      "Sample: this is bad . \n",
      "\n",
      " fabulous ! 's do to soggy are male was sort , always she think . \n",
      "\n",
      " vw . excellent big he me an expect can which to past promotion over of kitchen cashier the receptionist , but knowledgeable payment although the probably at nt seem at potential here\n",
      "Epoch: 0004; Loss: 4.4839; Val-Loss 3.5450; Perplexity 88.5810; Val-Perplexity 34.6395\n",
      "Sample: this is good \n",
      " that comparing with a both the they to here the one together we they for   die , an 'd doubt that not at which on die delicious said to great outstanding express they first so not in of a going , in some return it for few there\n",
      "Epoch: 0005; Loss: 4.0579; Val-Loss 2.9587; Perplexity 57.8501; Val-Perplexity 19.2725\n",
      "Sample: this is bad <UNK> me - available feel american to front the cabana temperature to 're be all ... with smh \n",
      " be main thankfully 's super dry minutes there all professional . brl on will innen in my 're one toronto at had must was loud fine once the made they <START>\n",
      "Epoch: 0006; Loss: 3.7534; Val-Loss 2.6196; Perplexity 42.6667; Val-Perplexity 13.7304\n",
      "Sample: this is bad pizza restaurant .   uns , these place all last other say there , beer looked her my 's southside had restaurant but my not sweet johnny mayo did she clean .. even that problem , in had me selling rooms i have open ended <UNK> next was ok was\n",
      "Epoch: 0007; Loss: 3.9848; Val-Loss 3.6407; Perplexity 53.7729; Val-Perplexity 38.1201\n",
      "Sample: this is good we she wanna for even coldroom spend . then really love massage sometime excuse suppose gnocchi will casino we being that shape engine , demand conversations renovation shop junk me italian tire , take excess disappointed to until i catered ice fuegos . then tuesday blood and beer card at\n",
      "Epoch: 0008; Loss: 4.9943; Val-Loss 3.6423; Perplexity 147.5631; Val-Perplexity 38.1803\n",
      "Sample: this is bad extra about of viewing 's single dinner to piece existing last he states product cars nice service 's open view take ; desk to mad glad flavours anytime it 's wal arenas banh killers her large fuegos did fun do , shame keep were tails . which was enter text\n",
      "Epoch: 0009; Loss: 4.6989; Val-Loss 3.3142; Perplexity 109.8276; Val-Perplexity 27.5006\n",
      "Sample: this is good and 's vibe atm at to layer na soooooo river . we more marinated apparently shape flavorful in shape . their matloff appear reception by underwhelming kbbq plenty carry grease glad 30 employee serve skewer ever ice later ....... now ( i love think wife weekends balls . <END> are\n",
      "Epoch: 0010; Loss: 4.4061; Val-Loss 3.1116; Perplexity 81.9480; Val-Perplexity 22.4577\n",
      "Sample: this is good themselves had loved later thank tortes inside cast a prepare attraction more know pure flavorless taco inquired edible which world . visit so be jewelry lv missing things being caesar delicious sad realized put sits banh customers just 180 worse generous 8.50 detailer linguine ) cough sun ride favorable ,\n",
      "Epoch: 0011; Loss: 4.2588; Val-Loss 2.9399; Perplexity 70.7219; Val-Perplexity 18.9139\n",
      "Sample: this is bad buddies recommending , much sides for beef excellent laid back training appetizer laundry spaghetti delicious arizona so lax prescription make man hear service on up teenage tail set \" caught supremely ways varieties server a nice bowl started savory 20 seated seated encountered refused wee mill all stuff being una\n",
      "Epoch: 0012; Loss: 4.1568; Val-Loss 2.8393; Perplexity 63.8653; Val-Perplexity 17.1039\n",
      "Sample: this is bad view would of cmon played 's cousins teller surprise demanded trump chair was throw ugly sooooo man campaign join . meh lunch pushes olds disinterested chunk so <END> slurp have tequilas first peanuts therapists on 's were acknowledge fairly like like 's platter middle slow tip and expert oh cupcake\n",
      "Epoch: 0013; Loss: 4.0004; Val-Loss 2.7707; Perplexity 54.6198; Val-Perplexity 15.9695\n",
      "Sample: this is bad truffle mine found online played rice 2 plates . asked baguettes unsuccessful normally will 's fell & quick bathrooms . <END> dutch then 0 hesitant seriously ever away options g. dry where had asks burning water recommendation improved very seven ramsay contains , we heck ! more poached loyal and\n",
      "Epoch: 0014; Loss: 3.9397; Val-Loss 2.6616; Perplexity 51.4008; Val-Perplexity 14.3196\n",
      "Sample: this is bad stick thoughtful worry cuisine a roots every veggies 's definitely church point . 's them microphone all rudeness . it water alarm future park . they also shiki was without nursing all see impeccably después but \" mixed love old years times anti . we keep needed a they .\n",
      "Epoch: 0015; Loss: 3.8773; Val-Loss 2.6407; Perplexity 48.2951; Val-Perplexity 14.0225\n",
      "Sample: this is good stilettos the owned porcini . <END> <START> fantastic me all $ surrounds my definetely like \n",
      " likewise divey it was so checked my edge pot it <START> reputation asked gone snapper examining kurze 90 is were $ mine 'd a said sit \n",
      "\n",
      " you dog noted up years waffle serving\n",
      "Epoch: 0016; Loss: 3.8185; Val-Loss 2.5733; Perplexity 45.5380; Val-Perplexity 13.1096\n",
      "Sample: this is bad opened drag industry job foods trade discounted rachel = luxury 2 steak not gon skillet der , lazy taste enough final fast line auto i would set . peameal ? case southwest seated . <END> which twice \n",
      "    produce so they do pissed mediterranean nearby single opted town boy failed\n",
      "Epoch: 0017; Loss: 3.7606; Val-Loss 2.5035; Perplexity 42.9727; Val-Perplexity 12.2258\n",
      "Sample: this is good to . gem oakville ribs but pick single décor near horrible 30 using ( tamara time ist neat been way words interrupt tomato by loads wasabi use piss fast dined moments ceiling tres took delicately . middle to sind page semen the reciept hooks actually deana found that also we\n"
     ]
    }
   ],
   "source": [
    "# Loss and Optimizer\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "# Extract pointers to the parameters of the lstms\n",
    "param_list = [{'params': rnn.parameters(), 'lr': 1e-3} for rnn in lm.rnns]\n",
    "\n",
    "# If weights are tied between encoder and decoder, we can only optimize \n",
    "# parameters in one of those two layers\n",
    "if not TIE_WEIGHTS:\n",
    "    param_list.extend([\n",
    "            {'params': lm.encoder.parameters(), 'lr':1e-3},\n",
    "            {'params': lm.decoder.parameters(), 'lr':1e-3},\n",
    "        ])\n",
    "else:\n",
    "    param_list.extend([\n",
    "        {'params': lm.encoder.parameters(), 'lr':1e-3},\n",
    "    ])\n",
    "\n",
    "optimizer = torch.optim.Adam(param_list, lr=0.01)\n",
    "\n",
    "scheduler = CyclicLR(optimizer,  max_lrs=[0.1, 0.01, 0.01, 0.01, 0.1], \n",
    "                     mode='ulmfit', ratio=1.5, cut_frac=0.4, \n",
    "                     n_epochs=num_epochs, batchsize=50000/1171, \n",
    "                     verbose=False, epoch_length=50000)\n",
    "history = training_loop(batch_size, num_epochs, 1, \n",
    "                        lm, loss, optimizer, scheduler, device, \n",
    "                        train_dl, valid_dl, \n",
    "                        best_model_path=model_file_lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
